library(tabulizer)
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")
library(tabulizer)
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
install.packages("rJava")
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
library(tabulizer)
library(DBI)
library(dbcooper)
library(dplyr)
# Librairies----
library(readtext)
library(quanteda)
# Data----
?readtext()
# Data----
readtext("MyData/.txt")
# Data----
readtext("MyData/*.txt")
# Data----
readtext("MyData/*.txt")
# Data----
text <- readtext("MyData/*.txt")
text
corp <- corpus(text)
corp
summary(corp)
?summary.corpus
tok <- tokens(corp)
kwic(tok)
kwic(tok, "migra")
kwic(tok, "migra ")
kwic(tok, "doc")
?kwic
kwic(tok, "doc", valuetype = "regex")
kwic(tok, "doc", valuetype = "regex")
kwic(tok, "doc", valuetype = "regex")
kwic(tok, "doc", valuetype = "regex")
kwic(tok, "doc", valuetype = "regex")
kwic(tok, "doc", valuetype = "regex")
kwic(tok, "doc", valuetype = "cultur")
kwic(tok, "culture", valuetype = "regex")
kwic(tok, "cultur", valuetype = "regex")
kwic(tok, "migr", valuetype = "regex")
# Librairies----
library(readtext)
library(quanteda)
# Data----
text <- readtext("MyData/*.txt")
corp <- corpus(text)
summary(corp)
tok <- tokens(corp)
kwic(tok, "corp", valuetype = "regex")
kwic(tok, "corp")
kwic(tok, "migr", valuetype = "regex")
kwic(tok, "corp", valuetype = "regex")
kwic(tok, "border", valuetype = "regex")
mon_dfm <- dfm(tok, remove = stopwords("en"))
mon_dfm <- dfm(tok) %>%
dfm_remove(stopwords("en"))
topfeatures(mon_dfm, 10)
?dfm_remove
?dfm
tok <- tokens(corp, remove_punct = TRUE)
kwic(tok, "border", valuetype = "regex")
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en"), )
tok <- tokens(corp, remove_punct = TRUE)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en"))
topfeatures(mon_dfm, 10)
quanteda::stopwords()
?quanteda::stopwords()
stopwords_getsources()
stopwords::stopwords_getsources()
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso"))
topfeatures(mon_dfm, 10)
textplot_wordcloud(mon_dfm, min_count = 6, random_order = FALSE, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
library(quanteda.textplots)
textplot_wordcloud(mon_dfm, min_count = 6, random_order = FALSE, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
?tokens_wordstem()
SnowballC::getStemLanguages()
tok <-
tokens(corp, remove_punct = TRUE) %>%
tokens_wordstem("french")
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso"))
# Top words
topfeatures(mon_dfm, 10)
tok <-
tokens(corp, remove_punct = TRUE) %>%
tokens_wordstem("english")
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso"))
# Analyse----
# Summary of all corpus
summary(corp)
# Top words
topfeatures(mon_dfm, 10)
library(quanteda.textstats)
# Word similarity
textstat_simil(dfm)
# Word similarity
textstat_simil(mon_dfm)
# Word similarity
textstat_simil(mon_dfm) %>%
as.list()
# Word similarity
textstat_simil(mon_dfm) %>%
as.list() %>%
.$ara_2015 %>%
dotchart()
# Arbre hiérarchique de distance
textstat_dist(mon_dfm)
# Arbre hiérarchique de distance
textstat_dist(mon_dfm) %>%
as.dist()
# Arbre hiérarchique de distance
textstat_dist(mon_dfm) %>%
as.dist() %>%
hclust()
# Arbre hiérarchique de distance
clust <-
textstat_dist(mon_dfm) %>%
as.dist() %>%
hclust()
clust$labels <- docnames(mon_dfm)
plot(clust)
library(quanteda.textmodels)
# Analyse de correpsondance
res <- textmodel_ca(mon_dfm)
res
res %>% class()
factoextra::fviz_ca(res)
library(factoextra)
fviz_ca_col()
fviz_ca_col(mon_dfm)
fviz_ca_col(res)
fviz_ca_row(res)
FactoMineR::HCPC(res, nb.clust = -1)
class(res)
class(res) <- append(class(res, "CA"))
append(class(res, "CA"))
append(class(res), "CA")
class(res) <- append(class(res), "CA")
class(res)
FactoMineR::HCPC(res, nb.clust = -1)
res
HCPC(res)
FactoMineR::HCPC(res)
fviz_cluster(res)
fviz_nbclust(res)
fviz_contrib()
fviz_contrib(res)
# Analyse de correpsondance
res <- textmodel_ca(mon_dfm)
fviz_ca_row(res)
fviz_contrib(res)
fviz_contrib(res, choice = "col")
fviz_contrib(res, choice = "col", axes = 1, top = 10)
fviz_contrib(res, choice = "col", axes = 2, top = 10)
?dfm_remove
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex")
# Wordcloud
textplot_wordcloud(mon_dfm,
min_count = 6,
random_order = FALSE,
rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
# Word similarity
textstat_simil(mon_dfm) %>%
as.list() %>%
.$ara_2015 %>%
dotchart()
# Arbre hiérarchique de distance
clust <-
textstat_dist(mon_dfm) %>%
as.dist() %>%
hclust()
clust$labels <- docnames(mon_dfm)
plot(clust)
# Analyse de correpsondance
res <- textmodel_ca(mon_dfm)
fviz_ca_row(res)
fviz_contrib(res, choice = "col", axes = 2, top = 10)
fviz_contrib(res, choice = "col", axes = 1, top = 10)
fviz_dend(res)
fviz_eig(res)
fviz_cos2(res)
fviz_ca_col(res)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https.*", valuetype = "regex")
# Analyse de correpsondance
res <- textmodel_ca(mon_dfm)
fviz_ca_row(res)
fviz_ca_col(res)
# Analyse de correpsondance
res <- textmodel_ca(mon_dfm)
fviz_ca_col(res)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https?.*", valuetype = "regex")
# Analyse de correpsondance
res <- textmodel_ca(mon_dfm)
fviz_ca_col(res)
res
# Analyse de correspondance
res <- textmodel_ca(mon_dfm)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https?.*", valuetype = "regex") %>%
dfm_trim(min_termfreq = 10)
# Analyse de correspondance
res <- textmodel_ca(mon_dfm)
fviz_ca_col(res)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https?.*", valuetype = "regex") %>%
dfm_trim(min_termfreq = 15)
# Analyse de correspondance
res <- textmodel_ca(mon_dfm)
fviz_ca_col(res)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https?.*", valuetype = "regex") %>%
dfm_trim(min_termfreq = 20)
# Analyse de correspondance
res <- textmodel_ca(mon_dfm)
fviz_ca_col(res)
mon_dfm <-
dfm(tok) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https?.*", valuetype = "regex") %>%
dfm_trim(min_termfreq = 50)
# Analyse de correspondance
res <- textmodel_ca(mon_dfm)
fviz_ca_col(res)
fviz_ca_row(res)
fviz_cos2(res)
fviz_cos2(res, "col")
fviz_cos2(res, "col", top = 10)
res$sv
fviz_contrib(res, choice = "col", axes = 1, top = 10)
# Look up for specific words
kwic(tok, "cultur", valuetype = "regex")
# Look up for specific words
kwic(tok, "cultur", valuetype = "regex")
# Look up for specific words
kwic(tok, "\\bcultur", valuetype = "regex")
tok1 <-
tokens(corp, remove_punct = TRUE)
tok2 <-
tokens(corp, remove_punct = TRUE) %>%
tokens_wordstem("english")
mon_dfm <-
dfm(tok2) %>%
dfm_remove(stopwords("en", source = "stopwords-iso")) %>%
dfm_remove("\\d+", valuetype = "regex") %>%
dfm_remove("https?.*", valuetype = "regex") %>%
dfm_trim(min_termfreq = 50)
# Look up for specific words
kwic(tok1, "\\bcultur", valuetype = "regex")
# Wordcloud
textplot_wordcloud(mon_dfm,
min_count = 6,
random_order = FALSE,
rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
dfm
mon_dfm
mon_dfm$docs
mon_dfm[,1]
mon_dfm[1]
mon_dfm[1,]
mon_dfm[c(4, 7, 11),]
mon_dfm[c(4, 7, 11),] %>%
textplot_wordcloud(comparison = TRUE)
# À ma manière----
text
# À ma manière----
library(tidytext)
text %>%
unnest_tokens("words")
text %>%
unnest_tokens("words", text)
text %>%
unnest_tokens("words", text, drop = TRUE)
library(stringr)
text %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"))
library(dplyr)
text %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"))
text %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}"))
text %>%
unnest_tokens("words", text, drop = TRUE) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}"))
text %>%
unnest_tokens("words", text) %>%
select(-text)
text %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}"))
text %>%
mutate(text = str_remove_all(text, "\\d+|https?.*")) %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}"))
text %>%
mutate(text = str_remove_all(text, "\\d+|https?.*")) %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}")) %>%
filter(!(words %in% stopwords("en", "stopwords-iso")))
tokens <- text %>%
mutate(text = str_remove_all(text, "\\d+|https?.*")) %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}")) %>%
filter(!(words %in% stopwords("en", "stopwords-iso")))
tok3 <-
text %>%
mutate(text = str_remove_all(text, "\\d+|https?.*")) %>%
unnest_tokens("words", text) %>%
mutate(doc_id = str_remove_all(doc_id, "\\.txt"),
year = str_extract(doc_id, "\\d{4}")) %>%
filter(!(words %in% stopwords("en", "stopwords-iso")))
rm(tokens)
tok3
library(ggplot2)
tok3 %>%
count(doc_id, words)
tok3 %>%
count(doc_id, words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, words, fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id)
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, words, fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id)
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, words, fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free")
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, reorder_within(words, n, doc_id), fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free")
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, reorder_within(words, n, doc_id), fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free") +
scale_x_reordered()
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, reorder_within(words, n, doc_id), fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free") +
scale_y_reordered()
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, reorder_within(words, n, doc_id), fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free") +
scale_y_reordered() +
theme(legend.position = "none")
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
bind_tf_idf(words, doc_id, n)
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
bind_tf_idf(words, doc_id, n) %>%
slice_max(tf_idf, n = 10) %>%
ggplot(aes(tf_idf, reorder_within(words, tf_idf, doc_id), fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free") +
scale_y_reordered() +
theme(legend.position = "none")
tok3 %>%
group_by(doc_id) %>%
count(words) %>%
slice_max(n, n=10) %>%
ggplot(aes(n, reorder_within(words, n, doc_id), fill = doc_id)) +
geom_col() +
facet_wrap(~doc_id, scales = "free") +
scale_y_reordered() +
theme(legend.position = "none")
source("C:/Users/Vestin/Desktop/CODE/R/Projets/Text_mining/blog-post-frontex/Scripts/f_collect.R", echo=TRUE)
get.all.pdf()
